---
title: 'Deep Learning for Object Detection'
date: 2016-11-21
permalink: /posts/2016/11/blog-post-5/
tags:
  - Deep Learning
  - Object Detection
  - R-CNN
---

**This is an original article. Please indicate reference if reproduced.**  
**本文部分内容参考自[1](http://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=2650324780&idx=1&sn=1579155aacc83e991a9a39b142a7a0b8&mpshare=1&scene=24&srcid=1005cJnBqAr84LfExJk9fxhm#rd),[2](http://www.52ml.net/20287.html)**

#### 摘要

作为物体检测领域的权威，PASCAL VOC数据集挑战赛在之前几年的成绩基本达到稳定。2013年基于区域提名的深度学习算法R-CNN首次将该领域的检测精度提高到新的高度，在短短的三年时间里，不断有新的深度学习方法提出，PASCAL VOC数据集上的检测精度和检测速度也在不断的刷新。本文从基于区域提名和端到端两个角度介绍了近三年最具代表性的算法，包括R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、YOLO和SSD。

#### 关键字

物体检测；深度学习；卷积网络；R-CNN

## 1 介绍

传统的深度学习监督算法主要用于物体的分类，而在PASCAL、ILSVRC等竞赛及实际应用中，我们还需要实现物体的定位及识别，物体的位置通常由边框(bounding box)标出。在实际当中，图像中通常含有多个物体，那么我们也需要分别对其定位和分类，及多目标的物体检测。  

目标检测对于人类来说并不困难，通过对图片中不同颜色模块的感知很容易定位并分类出其中目标物体，但对于计算机来说，面对的是RGB像素矩阵，从图像中很难直接得到像狗或猫这样子的抽象概念并定位其位置，再加上有时候多个物体和杂乱的背景混杂在一起，目标检测更加困难。物体的定位与识别是计算机视觉领域最基本且最具有挑战性的问题。在传统视觉领域，目标检测就是一个非常热门的研究方向，一些特定目标的检测，比如人脸检测和行人检测已经有非常成熟的技术了。普通的目标检测也有过很多的尝试，但是效果总是差强人意。  
传统的目标检测一般使用滑动窗口的框架，主要包括三个步骤：  

1. 利用不同尺寸的滑动窗口框住图中的某一部分作为候选区域；
2. 提取候选区域相关的视觉特征。比如人脸检测常用的Harr特征，行人检测和普通目标检测常用的HOG特征等；
3. 利用分类器进行识别，比如常用的SVM模型。  

多尺度形变部件模型DPM(Deformable Part Model）[1]是出类拔萃的，连续获得PASCAL VOC 2007到2009的检测冠军。DPM把物体看成了多个组成的部件(比如人脸的鼻子、嘴巴等)，用部件间的关系来描述物体，这个特性非常符合自然界很多物体的非刚体特征。DPM可以看作是HOG+SVM的扩展，很好的继承了两者的优点，在人脸检测、行人检测等任务上取得了不错的效果，但是DPM相对复杂，检测速度也较慢，从而也出现了很多改进的方法。  

CNNs在90年代被大量的使用，但是很快，其流行势头被支持向量机所取代。在2012年，Krizhevsky等人重拾对CNNs的兴趣，并且在ILSVRC的图像分类比赛中展现出了重大的突破[2]。他们的成功得益于使用了1.2百万张有标签的图像进行训练，以及一些用于CNNs的技巧。CNNs的显著结果在ILSVRC 2012 workshop中引起了巨大争论，问题也由此转向了如何将CNN应用到PASCAL VOC Challenge的物体检测任务中。  

基于深度学习的目标检测发展起来后，其实效果也一直难以突破。比如文献[3]中的算法在VOC 2007测试集合上的mAP只能30%多一点，文献[4]中的OverFeat在ILSVRC 2013测试集上的mAP只能达到24.3%。2013年R-CNN诞生了，VOC 2007测试集的mAP被提升至48%，2014年时通过修改网络结构又飙升到了66%，同时ILSVRC 2013测试集的mAP也被提升至31.4%。  

R-CNN(Region-based Convolutional Neur-al Networks)是一种结合区域提名(Region Proposal)和卷积神经网络(CNN)的目标检测方法。Ross Girshick在2013年的开山之作[5]奠定了这个子领域的基础，这篇论文后续版本发表在CVPR 2014[6]，期刊版本发表在PAMI 2015[2]。许多基于R-CNN的方法接踵而来，大部分都是将传统方法与R-CNN结合起来。目前，深度学习相关的目标检测方法也可以大致分为两派：  

1. 基于区域提名的，如R-CNN[1]、SPP-net[7]、Fast R-CNN[8]和Faster R-CNN[9]；
2. 端到端（End-to-End），无需区域提名的，如YOLO[10]和SSD[11]。  

本文接下来将按如下架构进行组织：第2节主要讲述有关基于区域提名的物体检测的核心思想，第3节是端到端的方法，第4节进行总结。


## 2 基于区域提名的方法

本节主要内容为基于区域提名的物体检测方法，包括R-CNN、SPP-net、Fast R-CNN和Faster R-CNN。

### 2.1 R-CNN

早期的物体检测算法主要利用了滑动窗口的思想，这种办法最大的弊端就是滑窗的长宽比的问题，由于物体在图像中可能呈现出多种尺度和长宽比，因此枚举各种可能的矩形框就使得复杂度大大增加。R-CNN[2]则采用了Selective Search。其检测的步骤如下：  

首先，通过Selective Search从原始图片中提取2000个左右的候选框，这些候选框中可能存在感兴趣的物体。然后将这些候选框归一化为统一的比例(原文中为$227 \times 227$)，并输入到CNN网络中提取特征。最后使用边框回归来调整边框的大小与位置，使用SVM分类器对调整后的边框图像进行分类与识别。  

![图1 R-CNN系统流程图](http://ww1.sinaimg.cn/large/535663c3gw1fa11c15tltj20fw05mdgz.jpg)  

R-CNN的训练分为三步，首先训练CNN，包括使用ImageNet分类数据进行预训练，以及使用PASCAL VOC数据进行微调。然后保持CNN部分不变，使用CNN提取的特征分别训练边框回归器和SVM分类器。文中之所以使用SVM而不是softmax是因为训练CNN时和SVM时的正样本标准有差异。在CNN中的正样本是覆盖Groudtruth到达50%以上的边框，而SVM的正样本是Groudtruth的真正类别。之所以分为这两个部分也是因为物体检测本身分为了定位和识别两个子任务。此外softmax在训练时是随机的选取负样本的，而由于物体不可能均匀的出现在并且分布在每张图像上，物体检测这个任务中本身就存在数据不平衡的现象，SVM在子集中使用了“hard negative”，即更有代表性的样本点，使得分类效果更好一些。  
虽然R-CNN成功地实现了深度学习在物体检测任务上的应用，并且在检测精度上得到了很大的提高，但是R-CNN存在很多缺点：  
1. 重复计算：通过Selective Search选出来的2000个左右候选框中仍然有很多是重叠的，每个候选框都需要通过CNN进行卷积运算，如此产生了大量耗时的重复计算；
2. SVM线性模型在标注数据不多的情况下并不是最好的选择；
3. R-CNN的训练分为多个步骤，十分繁琐，中间数据还需要单独保存。
4. 训练的时间复杂度和空间复杂度都很高，通过CNN得到的特征需要先保存在硬盘上，这些特征需要几百G的存储空间。
5. 上面所述的缺点导致R-CNN处理一张图片十分地慢，在GPU上需要13秒，在CPU上则需要53秒。

### 2.2 SPP-net

CNN网络在输入图像之前都需要将图像调整为固定的比例与尺寸，这主要是由于全连接层造成的。第一层全连接的数量决定于它之前的卷积层，而只有将图像尺寸与比例固定下来才能在最后的卷积层得到数量和比例一致的特征图。  

SPP-net[7]巧妙地在卷积层与全连接层之间加入了空间金字塔池化层(Spatial Pyramid Pooling)，使得不论输入图像的尺寸与比例如何都能在最后得到长度固定的特征向量。此外SPP-net还提出了卷积层共享的概念，使得整个算法的时间复杂度大幅缩减。

![图2 SPP-net空间金字塔池化层结构](http://ww4.sinaimg.cn/large/535663c3gw1fa11i5kdd9j20bd0853z5.jpg)

图2所示为SPP-net空间金字塔池化层结构。首先，使用Selective Search对一张任意尺寸的图像提取候选框，然后将该图像传入CNN，在CNN的最后一个卷积层得到特征图，根据原图与特征图的映射关系，可以确定候选框在特征图中的位置。对于每个特征图上的候选框，分别划分为$4 \times 4$，$2 \times 2$和$1 \times 1$的块，每个块使用max-pooling，这样我们就从卷积层得到了固定长度的特征向量，之后再将该向量交给后面的网络结构进行处理。  

空间金字塔池化层的提出使得神经网络结构可以输入任意尺寸的图像，同时共享卷积层也大大加快了整个算法的速度，但是SPP-net的训练仍然是分为了多个阶段，过程繁琐。而且SPP-net在微调时固定了卷积层网络，只对全连接层进行微调，而对于一个新的任务，对整个网络的微调显然也是有必要的。

### 2.3 Fast R-CNN

针对SPP-net的不足，Girshick Ross很快又提出了Fast R-CNN[8]。其主要使用多任务的方式并行训练分类和回归问题，使得训练过程变为单步。此外，Fast R-CNN还使用了SVD方法来分解全连接层的矩阵参数，将其压缩为两个规模较小的全连接层，使得总参数变少，速度更快。  

![图3 Fast R-CNN结构框架](http://ww4.sinaimg.cn/large/535663c3gw1fa15e3dsxwj20gm06iabv.jpg)

如图3所示为Fast R-CNN的结构框架，首先，与SPP-net相同，将整张图像输入给CNN提取特征图，根据原图与特征图之间的映射关系确定特征图中的候选框，即感兴趣区域。然后使用Rol Pooling来提取候选框的特征，这里的Rol Pooling简化了之前的空间金字塔池化层，只采用的一层结构。通过全连接层之后得到候选框的特征向量，将特征向量分别输出给分类器与边框回归器。这里使用了多任务的方式来将分类器与边框回归器联系起来，具体的主要是损失函数：

$$
L(p,u,t^u,v) = L\_{cls}(p,u)+\lambda[u \geq 0]L\_{loc}(t\_u,v)
$$

其中$p=(p\_0,...,p\_K)$代表了预测的类别概率，$t^u=(t^u\_x,t^u\_y,t^u\_w,t^u\_h)$表示预测的边框的相位，$u$、$v$则表示相应的真实值。此外，文中还提出了BP算法通过Rol Pooling时的具体算法，如下：  

$$
\frac{\partial L}{\partial x\_i}=\sum\_{r}\sum\_{j}[i=i^\*(r,j)]\frac{\partial L}{\partial y\_{rj}}
$$

其中$x\_i$表示第个由Rol Pooling选出来的特征值，$y\_{rj}$表示该特征图上第$r$个候选框的第$j$个输出。$[i=i^\*(r,j)]$表示$x\_i$是否为第$r$个候选框的第$j$个子块的最大值输出。  

Fast R-CNN实现了更加快速与精确的检测结果，但是整个任务仍然是分多阶段进行的，而且大部分时间都耗费在提取候选框的过程中。Faster R-CNN就是主要针对提取候选框而设计的物体检测框架


### 2.4 Faster R-CNN

在上述方法中，已经通过共享卷积层、多任务的损失函数等方法实现了快速、准确的物体检测系统，但是候选框的提取仍然是耗时的一个部分，[9]就介绍了使用深度学习来提取候选框的方法，称作RPNs(Region Proposal Networks)。

![图4 RPNs结构框架](http://ww4.sinaimg.cn/large/535663c3gw1fa15jv1gqzj20b106naam.jpg)

如图4所示，首先由CNN得到一张图的特征图。在特征图上使用$3 \times 3$的滑窗提取特征，每个滑动一个位置，使用全连接的方式提取一个256维的特征向量，之后将这个特征向量分别输给分类器和边框回归器(这里的分类器只实现二分类，即有物体和没有物体)。为了适应不同大小和比例的物体，我们将滑窗的中心点称为锚点，每个锚点对应原图中3种尺度和3中长宽比的候选框，这样每个锚点也就可以在原图图上枚举到9个候选框，9个候选框对应的特征皆是上述特征图上$3 \times 3$的滑窗位置。

![图5 Faster R-CNN结构框架](http://ww1.sinaimg.cn/large/535663c3gw1fa15kg4e09j20b80aw3z7.jpg)

Faster R-CNN的训练分为如下几个步骤。首先使用ImageNet的数据对RPNs进行预训练，并对候选框提取部分进行微调。之后使用上一阶段中RPNs推荐的候选框训练Fast R-CNN。然后再用训练好的候选框重新初始化RPNs，此时保持卷积层不变。最后，固定卷积层微调Fast R-CNN的全连接层。  

Faster R-CNN通过重新设计候选框提取从而增加的检测的速度，并且真正实现了端到端的解决方法，但是预先获取候选框，计算量仍然较大，Fast R-CNN还是达不到实时的检测。


## 3 基于回归思想的端到端方法

本节主要内容为基于回归思想的物体检测方法，其实现了端到端的物体检测，包括YOLO，SSD。

### 3.1 YOLO

在第2节中提到的方法都是基于候选框和R-CNNs的模式，YOLO(You Only Look Once)提出了一种端到端的网络来实现物体检测。

![图6 YOLO算法流程](http://ww3.sinaimg.cn/large/535663c3gw1fa15ulxpz5j20be078q3z.jpg)

YOLO的算法流程如图6所示，首先将输入图像分为$7\times7$的网格，对于每个网格，我们预测两个边框，每个边框包括目标的置信度和每个边框区域中包含物体的类别概率。这样一共可以预测出$7\times7\times2$个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后用非极大值抑制(NMS)去除冗余窗口。  

YOLO的网络结构如图7所示。前边是连续的卷积层，在卷积层之后接了一个4096维的全连接层，然后是一个$7\times7\times30$维的张量。$7\times7$就表示划分的网格数，在每个网格代表的位置上，还需要预测两个边框的位置(中心点坐标和长宽)和置信度以及该网格中预测的物体类别(PASCAL VOC共20个类别)，所以组成一个$(4+1)\times2+20=30$维的向量，这样可以利用前边4096维的全图特征直接在每个网格预测出目标检测需要的信息。  

YOLO方法将目标检测转换成一个回归问题，大大提高了检测速度。由于每个网络预测窗口时使用了全图的信息(上下文信息)，使得False Positive大大下降，但是图像划分为固定的$7\times7$网格使得检测精度很低.。


### 3.2 SSD

针对YOLO的缺点，Liu W等提出了SSD(Single Shot MultiBox Detector)。如图7所示，图7(a)所示是带有两个Ground Truth边框的输入图片，图7(b)和(c)分别表示将特征图分为$8\times8$和$4\times4$的网格，前者适合检测尺寸较小的目标，如图片中的猫，后者适合检测尺寸较大的目标，比如说图片中的狗。每个格子都对应一些固定大小的Box，文中称为Default Box，用来框出物体位置，在训练中Ground Truth会赋予给某一固定的Box，比如图7(b)和(c)中的蓝框和红框。  

![图7 SSD多尺度网格下提取边框示意图
](http://ww2.sinaimg.cn/large/535663c3gw1fa160h59zwj20pr08mq5z.jpg)

![图8 SDD结构框架
](http://ww4.sinaimg.cn/large/535663c3gw1fa161l61v4j20zr0aujv4.jpg)

SSD的网络分为两个部分，前一部分是用于图像分类的标准化网络结构，后一部分是用于检测的多尺度特征映射层。


## 4 总结

深度学习在物体检测中的应用起步时间并不长，但是在短短的三年时间里，该领域得到了迅猛的发展，不论是在检测的精度上，还是在算法的速度上都在不断地提高。从上面的讲解中可以看出，深度学习的论文都偏向工程性，思想并不难理解，要做出卓越的成果，关键在于动手实验的过程，这也提醒我们工科生要在不断的实践对于科研和工程项目的重要性。


#### 参考文献

[1] Felzenszwalb P F, Girshick R B, McAllester D, et al. Object detection with discriminatively trained part-based models[J]. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2010, 32(9): 1627-1645.  

[2] R. Girshick, J. Donahue, T. Darrell, J. Malik. Region-Based Convolutional Networks for Accurate Object Detection and Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, May. 2015.  

[3] C. Szegedy, A. Toshev, D. Erhan. Deep Neural Networks for Object Detection. Advances in Neural Information Processing Systems 26 (NIPS), 2013.  

[4] P. Sermanet, D. Eigen, X.Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.  

[5] R. Girshick, J. Donahue, T. Darrell, J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. ImageNet Large-Scale Visual Recognition Challenge workshop, ICCV, 2013.  

[6] R. Girshick, J. Donahue, T. Darrell, J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.  

[7] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.  

[8] Girshick, R. Fast R-CNN. ICCV 2015.  

[9] S. Ren, K. He, R. Girshick, J. Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Advances in Neural Information Processing Systems 28 (NIPS), 2015.  

[10] Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR. (2016)  

[11] Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector[J]. arXiv preprint arXiv:1512.02325, 2015.


-------------
**如有任何相关问题欢迎评论或者邮件讨论<15120452@bjtu.edu.cn>**
